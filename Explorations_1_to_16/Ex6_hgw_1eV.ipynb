{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ex6. 작사가 인공지능 \n",
        "버전확인\n",
        "\n"
      ],
      "metadata": {
        "id": "jgobsI-LUtkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtpnWny2uhf4",
        "outputId": "09aac89b-034d-47d5-f1cd-f16acc9c6f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 함수 정의"
      ],
      "metadata": {
        "id": "VQxLmxlbVCqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIS9GxCJ2BdG"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
        "  \n",
        "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "es6RWSte2G8e"
      },
      "outputs": [],
      "source": [
        "def tokenize(corpus):\n",
        "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000,  # 전체 단어의 개수 \n",
        "            filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
        "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
        "\n",
        "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
        "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7w0FRUaA3zW4"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
        "\n",
        "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
        "        test_tensor = tf.concat([test_tensor, \n",
        "        tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 파일을 읽기 전용으로 불러오기"
      ],
      "metadata": {
        "id": "u6v1GwANfTGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylZ7Zvm4vH6B",
        "outputId": "ab781cff-b625-4a62-be27-a8832ee99af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "txt_file_path ='/content/drive/MyDrive/lyricist/*.txt'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "     with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "소스부와 타겟부 분리"
      ],
      "metadata": {
        "id": "aMdmWogwfzsi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHhYCd_svUuV",
        "outputId": "bff387d8-6f85-442c-e138-587e61b1eea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  23  77 ...   0   0   0]\n",
            " [  2  42  26 ...   0   0   0]\n",
            " [  2  23  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7ffa28397b10>\n",
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "        \n",
        "    corpus.append(preprocess_sentence(sentence))\n",
        "        \n",
        "corpus[:10]\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <END>가 아니라 <pad>일 가능성이 높습니다.\n",
        "tgt_input = tensor[:, 1:]   # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=2020)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('enc_train : ',enc_train.shape)\n",
        "print('enc_val : ',enc_val.shape)\n",
        "print('dec_train : ',dec_train.shape)\n",
        "print('dec_val : ',dec_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiZmE4Q6VY5m",
        "outputId": "205585e8-039c-4282-9787-15c16469fff9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc_train :  (140599, 14)\n",
            "enc_val :  (35150, 14)\n",
            "dec_train :  (140599, 14)\n",
            "dec_val :  (35150, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 만들고 훈련"
      ],
      "metadata": {
        "id": "VgzHb4Fxf53h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSQuKHehvlrE",
        "outputId": "e9d11007-73bf-4834-cc6d-26adbba99b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "549/549 [==============================] - 236s 424ms/step - loss: 3.3404 - accuracy: 0.4792 - val_loss: 2.9727 - val_accuracy: 0.5061\n",
            "Epoch 2/10\n",
            "549/549 [==============================] - 233s 424ms/step - loss: 2.7690 - accuracy: 0.5232 - val_loss: 2.7002 - val_accuracy: 0.5346\n",
            "Epoch 3/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 2.4185 - accuracy: 0.5579 - val_loss: 2.5202 - val_accuracy: 0.5612\n",
            "Epoch 4/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 2.0973 - accuracy: 0.5981 - val_loss: 2.3955 - val_accuracy: 0.5845\n",
            "Epoch 5/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 1.8093 - accuracy: 0.6424 - val_loss: 2.3152 - val_accuracy: 0.6051\n",
            "Epoch 6/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 1.5663 - accuracy: 0.6849 - val_loss: 2.2674 - val_accuracy: 0.6226\n",
            "Epoch 7/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 1.3716 - accuracy: 0.7225 - val_loss: 2.2516 - val_accuracy: 0.6356\n",
            "Epoch 8/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 1.2233 - accuracy: 0.7529 - val_loss: 2.2587 - val_accuracy: 0.6425\n",
            "Epoch 9/10\n",
            "549/549 [==============================] - 234s 425ms/step - loss: 1.1148 - accuracy: 0.7769 - val_loss: 2.2761 - val_accuracy: 0.6478\n",
            "Epoch 10/10\n",
            "549/549 [==============================] - 233s 425ms/step - loss: 1.0403 - accuracy: 0.7933 - val_loss: 2.3113 - val_accuracy: 0.6501\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa26269950>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1  \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.drop1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.drop1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "    \n",
        "embedding_size = 2432\n",
        "hidden_size = 2432\n",
        "\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "model(src_sample)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit(dataset, epochs=10, validation_data=val_dataset, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "작문 결과"
      ],
      "metadata": {
        "id": "jbKnU9QggEus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MlQJhNyfv4XN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1bf30bb-07a4-4217-89e5-e1176520e8c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love it when you call me big poppa <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "회고 : 결론부터 말하자면 validation_loss가 2.2 이하로 다다르지 못했네요. BATCH_SIZE와 hidden_size, embedding size, learning rate을 다양하게 바꿔가며 시간을 들여 시도해봤지만 결국 원하는 결과를 얻지 못했습니다. 그러나 언어처리를 해봤다는 측면에서 먼가 배운 것 같은 느낌도 들고 컴퓨터가 스스로 문장을 내놓는 것이 (맘에 들진 않지만) 신기하기도 합니다."
      ],
      "metadata": {
        "id": "lpIJIPLOgMtg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Ex6_hgw_1eV.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
