{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddae3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "    with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "# Upload your kaggle.json file with your username and your Kaggle API token.\n",
    "files.upload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure the kaggle.json file is present. \n",
    "!ls -lha kaggle.json\n",
    "# Next, install the Kaggle API client. \n",
    "!pip install -q kaggle\n",
    "# The Kaggle API client expects this file to be in ~/.kaggle, \n",
    "# so move it there. \n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "# This permissions change avoids a warning on Kaggle tool startup. \n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/My\\ Drive/ConditionalVAE_DL_Project3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5780f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50380bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rom celeba import CelebADataset\n",
    "\n",
    "# Training configuration\n",
    "learning_rate = 0.001\n",
    "train_size = 0.01\n",
    "batch_size = 32\n",
    "save_test_set = True # S# True: the test set image IDs and other useful information will be stored in a pickle file to further uses (e.g. Image_Generation.ipynb) \n",
    "\n",
    "\n",
    "dataset = CelebADataset(train_size = train_size, batch_size = batch_size, save_test_set = save_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f32343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "label_dim = 40\n",
    "image_dim = [64, 64, 3]\n",
    "latent_dim = 128\n",
    "beta = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec24cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ConvolutionalCondVAE import ConvCVAE, Decoder, Encoder\n",
    "\n",
    "# Model\n",
    "encoder = Encoder(latent_dim)\n",
    "decoder = Decoder()\n",
    "model = ConvCVAE(\n",
    "                encoder,\n",
    "                decoder,\n",
    "                label_dim = label_dim,\n",
    "                latent_dim = latent_dim,\n",
    "                beta = beta,\n",
    "                image_dim = image_dim)\n",
    "\n",
    "# Optiizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_root = \"./CVAE{}_{}_checkpoint\".format(latent_dim, beta)\n",
    "checkpoint_name = \"model\"\n",
    "save_prefix = os.path.join(checkpoint_root, checkpoint_name)\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint = tf.train.Checkpoint(module=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef18842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the latest checkpoint\n",
    "latest = tf.train.latest_checkpoint(checkpoint_root)\n",
    "\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "    print(\"Checkpoint restored:\", latest)\n",
    "else:\n",
    "  print(\"No checkpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from utils import train_step\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_latent_losses = []\n",
    "loss = []\n",
    "reconstruct_loss = []\n",
    "latent_loss = []\n",
    "\n",
    "step_index = 0\n",
    "n_batches = int(dataset.train_size / batch_size)\n",
    "n_epochs = 30\n",
    "\n",
    "print(\"Number of epochs: {},  number of batches: {}\".format(n_epochs, n_batches))\n",
    "\n",
    "# Epochs Loop\n",
    "for epoch in range(5):\n",
    "    start_time = time.perf_counter()\n",
    "    dataset.shuffle() # Shuffling\n",
    "\n",
    "    # Train Step Loop\n",
    "    for step_index, inputs in enumerate(dataset):\n",
    "        total_loss, recon_loss, lat_loss = train_step(inputs, model, optimizer)\n",
    "        train_losses.append(total_loss)\n",
    "        train_recon_errors.append(recon_loss)\n",
    "        train_latent_losses.append(lat_loss)\n",
    "\n",
    "        if step_index + 1 == n_batches:\n",
    "            break\n",
    "\n",
    "    loss.append(np.mean(train_losses, 0))\n",
    "    reconstruct_loss.append(np.mean(train_recon_errors, 0))\n",
    "    latent_loss.append(np.mean(train_latent_losses, 0))\n",
    "\n",
    "    exec_time = time.perf_counter() - start_time\n",
    "    print(\"Execution time: %0.3f \\t Epoch %i: loss %0.4f | reconstr loss %0.4f | latent loss %0.4f\"\n",
    "                        % (exec_time, epoch, loss[epoch], reconstruct_loss[epoch], latent_loss[epoch])) \n",
    "\n",
    "\n",
    "    # Save progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(save_prefix + \"_\" + str(epoch + 1))\n",
    "        print(\"Model saved:\", save_prefix)\n",
    "            \n",
    "# Save the final model                \n",
    "checkpoint.save(save_prefix)\n",
    "print(\"Model saved:\", save_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reconstruct_loss, 'g', marker ='o')\n",
    "plt.grid()\n",
    "plt.show();\n",
    "plt.plot(latent_loss, 'b', marker = 'o')\n",
    "plt.grid()\n",
    "plt.show();\n",
    "plt.plot(loss, 'r', marker ='o')\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
